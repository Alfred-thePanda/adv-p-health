{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- basically is erm KNN & Decision tree zoid, bzw. daccuracy ned hoch gnua\n",
    "- am besten in da final presentation dann erw√§hnen, wann classifiers entwickelt woan hand**\n",
    " \n",
    " Algorithms to try out: \n",
    "\n",
    "- SVM\n",
    "- SGD\n",
    "- SGB\n",
    "\n",
    "\n",
    "update performance conclusion automatically"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for columns containing 'DIABETE' to find target var\n",
    "\n",
    "diabate_cols = [col for col in df.columns if 'DIABETE' in col]\n",
    "diabate_cols.append([col for col in df.columns if 'diabete' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove colums containing NaN values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "# print(\"No. of columns containing null values\")\n",
    "# print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "# print(\"No. of columns not containing null values\")\n",
    "# print(len(df.columns[df.notna().all()]))\n",
    "\n",
    "# print(\"Total no. of columns in the dataframe\")\n",
    "# print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing target var from feature list\n",
    "target = df['diabetes']\n",
    "features = df.drop(['diabetes'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test data\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features_train.shape)\n",
    "# print(features_test.shape)\n",
    "# print(target_train.shape)\n",
    "# print(target_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing max_iter to reach the highest possible Accuracy\n",
    "\n",
    "# amount of trials (should be the same as the max value for max_iter, then all possibilities are tried out)\n",
    "MAX_EVALS = 20\n",
    "\n",
    "# One 'run' equals one fmin-execution where each run for a number of x trials\n",
    "SEARCH_SPACE = [hp.randint('max_iter',100)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function(max_iter):\n",
    "    max_iter = max_iter[0]\n",
    "    print(max_iter)\n",
    "    if max_iter == 0:\n",
    "        return 0\n",
    "    svm_classifier = make_pipeline(StandardScaler(), svm.SVC(max_iter=max_iter)).fit(features_train, target_train)\n",
    "    # svm_classifier = svm.SVC(kernel = \"linear\", max_iter = max_iter)\n",
    "    svm_predictions = svm_classifier.predict(features_test)\n",
    "    svm_accuracy = accuracy_score(target_test, svm_predictions)\n",
    "\n",
    "    print(f\"Accuracy : {100 * svm_accuracy}\")\n",
    "    return {'loss': - svm_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "\n",
    "- simple SVM: Best reachable Accuracy: 86.80559 %, with max_iter= 36\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy:  99.668 %, with max_iter = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Stocastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing max_iter to reach the highest possible Accuracy\n",
    "\n",
    "# amount of trials (should be the same as the max value for max_iter, then all possibilities are tried out)\n",
    "MAX_EVALS = 40\n",
    "\n",
    "# One 'run' equals one fmin-execution where each run for a number of x trials\n",
    "SEARCH_SPACE = [hp.randint('max_iter',100)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function(max_iter):\n",
    "    max_iter = max_iter[0]\n",
    "    if max_iter == 0:\n",
    "        return 0\n",
    "    print(max_iter)\n",
    "    # sgd_classifier = make_pipeline(StandardScaler(), SGDClassifier(max_iter=max_iter)).fit(features_train, target_train)\n",
    "    sgd_classifier = SGDClassifier(max_iter = max_iter).fit(features_train, target_train)\n",
    "    sgd_predictions = sgd_classifier.predict(features_test)\n",
    "    sgd_accuracy = accuracy_score(target_test, sgd_predictions)\n",
    "\n",
    "    print(f\"Accuracy : {100 * sgd_accuracy}\")\n",
    "    return {'loss': - sgd_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- simple SGD: Best reachable Accuracy:  87.33042784359463%, with max_iter= 74\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy:  100%, with max_iter = 11 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Stocastic Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing max_depth to reach the highest possible Accuracy\n",
    "\n",
    "# amount of trials (should be the same as the max value for max_iter, then all possibilities are tried out)\n",
    "MAX_EVALS = 10\n",
    "\n",
    "# One 'run' equals one fmin-execution where each run for a number of x trials\n",
    "SEARCH_SPACE = [hp.randint('max_depth',100)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function(max_depth):\n",
    "    max_depth = max_depth[0]\n",
    "    if max_depth == 0:\n",
    "        return 0\n",
    "    print(max_depth)\n",
    "    # sgb_classifier = make_pipeline(StandardScaler(), sgb_classifier = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, max_depth=max_depth, random_stat=0).fit(features_train, target_train)\n",
    "    sgb_classifier = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, max_depth=max_depth).fit(features_train, target_train)    \n",
    "    sgb_predictions = sgb_classifier.predict(features_test)\n",
    "    sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "    comp = target_test == sgb_predictions\n",
    "    print(Counter(comp))\n",
    "    print(f\"Accuracy : {100 * sgb_accuracy}\")\n",
    "    return {'loss': - sgb_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- simple SGB: Best reachable Accuracy:  87.33042784359463%, with max_iter= 74\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy:  100%, with max_iter = 11 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
