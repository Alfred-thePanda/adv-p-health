{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz \n",
    "\n",
    "from sklearn.model_selection import train_test_split#\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "px_template = \"simple white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/DIABETE4_health_indicators_BRFSS2015.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0],axis = 1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIABETE4', ['diabetes']]\n"
     ]
    }
   ],
   "source": [
    "# search for columns containing 'DIABETE' to find target var\n",
    "\n",
    "diabate_cols = [col for col in df.columns if 'DIABETE' in col]\n",
    "diabate_cols.append([col for col in df.columns if 'diabete' in col])\n",
    "print(diabate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of columns containing null values\n",
      "0\n",
      "No. of columns not containing null values\n",
      "176\n",
      "Total no. of columns in the dataframe\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "# remove colums containing string values\n",
    "df = df.dropna(axis=1)\n",
    "df.shape\n",
    "\n",
    "print(\"No. of columns containing null values\")\n",
    "print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "print(\"No. of columns not containing null values\")\n",
    "print(len(df.columns[df.notna().all()]))\n",
    "\n",
    "print(\"Total no. of columns in the dataframe\")\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing target var from feature list\n",
    "target = df['diabetes']\n",
    "# target = target.to_frame()\n",
    "features = df.drop(['diabetes'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test data\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349323, 175)\n",
      "(87331, 175)\n",
      "(349323,)\n",
      "(87331,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree vizualisation\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(model, \n",
    "                   feature_names=features_train.columns,  \n",
    "                   class_names='diabetes',\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(target_test, predictions)\n",
    "acc_percentage = accuracy * 100\n",
    "acc_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a cnn \n",
    "# model = MLPClassifier(hidden_layer_sizes=(50,20, 10), random_state=1,\n",
    "#               solver='adam')\n",
    "# model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing dimensionality \n",
    "# print(target_train.shape)\n",
    "# print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting for test data\n",
    "# predictions = model.predict([features_test])\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.Hyperopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df/DIABETE4_health_indicators_BRFSS2015.csv')\n",
    "features = df.drop('DIABETE4',axis=1)\n",
    "groundtruth = df['DIABETE4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable DIABETE4:\n",
    "#- 0 : no diabetes/ only during pregnancy\n",
    "#- 1 : prediabetes\n",
    "#- 2 : diabetes\n",
    "\n",
    "### Classifier: #######################################################################################################\n",
    "\n",
    "def no_diabetes_classifier(groundtruth) -> bool:\n",
    "    if (groundtruth == 0):\n",
    "        return (True)\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def diabetes_classifier(groundtruth) -> bool:     \n",
    "    if (groundtruth == 2.0):\n",
    "        return(True) \n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def classifier(features):\n",
    "    predictions = []\n",
    "    for i in range(0, len(features)):\n",
    "        # is it diabetes?\n",
    "        if ((diabetes_classifier(features[i]))):\n",
    "            predictions.append(2.0)\n",
    "        # is it no diabetes?\n",
    "        elif (no_diabetes_classifier(features[i])):\n",
    "            predictions.append(0.0)\n",
    "        # its neither, so patient is at risk\n",
    "        else: predictions.append(1.0)\n",
    "    # print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define performance metrics (True negatives, false negatives, true positives, false positives, recall, specificity, precision)\n",
    "    # recall = Sensitifity = True Positive Rate\n",
    "    # specificity = True Negative Rate\n",
    "    # precision = TP / (TP + FP)\n",
    "\n",
    "def calc_metrics(predictions, groundtruth):\n",
    "    # predictions = predictions\n",
    "    TP_no_diabetes = TN_prediabetes = TN_diabetes = TN_no_diabetes = TN_diabetes = TP_prediabetes = TP_diabetes = FP_no_diabetes = FN_diabetes = FN_prediabetes = FP_diabetes = FP_prediabetes = FN_no_diabetes = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if (predictions[i] == groundtruth[i]):\n",
    "            if (predictions[i] == 0):\n",
    "                TP_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif (predictions[i] ==1):\n",
    "                TP_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "            else : \n",
    "                TP_diabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                \n",
    "        elif (predictions[i] == 0):\n",
    "            if (groundtruth[i] == 1):\n",
    "                FP_no_diabetes += 1\n",
    "                FN_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif (groundtruth[i] == 2):\n",
    "                FP_no_diabetes += 1\n",
    "                FN_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                \n",
    "        elif (predictions[i] == 1):\n",
    "            if (groundtruth[i] == 0):\n",
    "                FP_prediabetes += 1\n",
    "                FN_no_diabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif(groundtruth[i] == 2):\n",
    "                FP_prediabetes += 1\n",
    "                FN_diabetes += 1\n",
    "                TN_no_diabetes+= 1\n",
    "                \n",
    "        elif (predictions[i] == 2):\n",
    "            if (groundtruth[i] == 0):\n",
    "                FP_diabetes += 1\n",
    "                FN_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "            elif(groundtruth[i] == 1):\n",
    "                FP_diabetes += 1\n",
    "                FN_prediabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "                \n",
    "    TP_total = TP_diabetes + TP_no_diabetes + TP_prediabetes \n",
    "    TN_total = TN_diabetes + TN_no_diabetes + TN_prediabetes\n",
    "    FP_total = FP_diabetes + FP_no_diabetes + FP_prediabetes\n",
    "    FN_total = FN_diabetes + FN_no_diabetes + FN_prediabetes \n",
    "    recall = TP_total/ (TP_total + FN_total)\n",
    "    specificity =  TN_total / (TN_total + FP_total)\n",
    "    precision = (TP_total / (TP_total + FP_total))\n",
    "    return (recall, specificity, precision)\n",
    "\n",
    "\n",
    "recall, specificity, precision = calc_metrics(predictions, groundtruth)\n",
    "print (recall, specificity, precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt for number of optimal components with most information/ number ob components\n",
    "\n",
    "### Constants ##############################################################################################################\n",
    "\n",
    "# amount of trials\n",
    "MAX_EVALS = 3\n",
    "\n",
    "# One 'run' equals one fmin-execution where each run for a number of x trials\n",
    "SEARCH_SPACE = [hp.uniform('default', 0, 1)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function(features):\n",
    "    predictions = classifier(groundtruth)\n",
    "    recall, specifity, precision = calc_metrics(predictions, groundtruth)\n",
    "    # print(f\"recall: {recall}\")\n",
    "    # print(f\"specificity: {specifity}\")\n",
    "    # specifity is maximized (= minimizes not recognized diabetes cases)\n",
    "    print(f\"Specifity (TN-Rate): {specifity}\")\n",
    "    return {'loss': - specifity , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimal Component configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df/DIABETE4_health_indicators_BRFSS2015.csv')\n",
    "features = df.drop('DIABETE4',axis=1)\n",
    "groundtruth = df['DIABETE4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standartizing and normalizing df\n",
    "feature_names = list(features.columns)\n",
    "\n",
    "x = df.loc[:,feature_names].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "# np.mean(x), np.std(x)\n",
    "\n",
    "## Format back into dfframe\n",
    "feature_columns = ['feature_names' + str(i) for i in range (x.shape[1])]\n",
    "normalized_df = pd.dfFrame(x, columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to wich number of features should be reduced? (max 79)\n",
    "n_components = 5 \n",
    "\n",
    "# actual PCA\n",
    "pca_df = PCA(n_components=n_components)\n",
    "principal_components = pca_df.fit_transform(x)\n",
    "\n",
    "column_names = []\n",
    "for i in range (0, n_components):\n",
    "    count = str(i)\n",
    "    column_names.append('PC'+count)\n",
    "\n",
    "column_names = np.array(column_names)\n",
    "print(column_names)\n",
    "\n",
    "components_DF = pd.dfFrame(df = principal_components, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dfframe with percentage each component adds (output is sorted)\n",
    "explained_var = pca_df.explained_variance_ratio_\n",
    "explained_percentage = 0\n",
    "\n",
    "explained_var_per = []\n",
    "for i in range (len(explained_var)):\n",
    "    explained_percentage += (explained_var[i] * 100)   \n",
    "    explained_var_per.append(explained_var[i] *100)\n",
    "\n",
    "explained_var_df = pd.dfFrame(explained_var_per)\n",
    "\n",
    "explained_var_df.columns =[\"% of Information\"]\n",
    "explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "# explained_var_df\n",
    "explained_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ##############################################################################################################\n",
    "\n",
    "# amount of trials\n",
    "MAX_EVALS = 2\n",
    "SEARCH_SPACE = [hp.uniform('number_of_components',0,20)]\n",
    "\n",
    "### df preperation ##############################################################################################################\n",
    "\n",
    "# Standartizing and normalizing df\n",
    "def df_prep(df):\n",
    "    feature_names = list(features.columns)\n",
    "    x = df.loc[:,feature_names].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    return x\n",
    "\n",
    "# PCA\n",
    "def pca (number_of_components):\n",
    "    pca_df = PCA(n_components=number_of_components)\n",
    "    explained_percentage = 0\n",
    "    explained_var_percentages = []\n",
    "    column_names = []\n",
    "    x = df_prep(df)\n",
    "    principal_components = pca_df.fit_transform(x)\n",
    "    \n",
    "    for i in range (0, number_of_components):\n",
    "        count = str(i)\n",
    "        column_names.append('PC'+count)\n",
    "\n",
    "    column_names = np.array(column_names)\n",
    "    explained_var = pca_df.explained_variance_ratio_\n",
    "\n",
    "    for i in range (len(explained_var)):\n",
    "        explained_percentage += (explained_var[i] * 100)   \n",
    "        explained_var_percentages.append(explained_var[i] *100)\n",
    "\n",
    "    explained_var_df = pd.dfFrame(explained_var_percentages)\n",
    "    explained_var_df.columns =[\"% of Information\"]\n",
    "    explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "    \n",
    "    return explained_percentage, number_of_components\n",
    "\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function_pca(number_of_components):\n",
    "    number_of_components = int(number_of_components[0])\n",
    "    #print(number_of_components)\n",
    "    explained_percentage, number_of_components = pca(number_of_components)\n",
    "    information_component_ratio = (explained_percentage / number_of_components)\n",
    "    return {'loss': - information_component_ratio , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function_pca,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "def cleanup_loss():\n",
    "    raw_loss = float(trials.best_trial['result']['loss'])\n",
    "    #raw_loss.dtype\n",
    "    print(raw_loss)\n",
    "    n = int(trials.best_trial['misc']['vals']['number_of_components'])\n",
    "    print (n)\n",
    "    clean__reached_percentage = raw_loss * n\n",
    "\n",
    "    return clean__reached_percentage\n",
    "\n",
    "\n",
    "\n",
    "perc = cleanup_loss()\n",
    "print(trials.best_trial)\n",
    "print(\"percentage for {n} coponents: {perc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ##############################################################################################################\n",
    "MAX_EVALS = 3\n",
    "n_components_fixed = 10\n",
    "\n",
    "### df preperation ##############################################################################################################\n",
    "\n",
    "# Standartizing and normalizing df\n",
    "\n",
    "def df_prep():\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    x = df.loc[:,feature_names].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    feature_columns = ['feature_names' + str(i) for i in range (x.shape[1])]\n",
    "    normalized_df = pd.dfFrame(x, columns=feature_columns)\n",
    "\n",
    "    return feature_names, feature_columns\n",
    "\n",
    "\n",
    "# actual PCA\n",
    "\n",
    "def pca (n_components):\n",
    "    pca_df = PCA(n_components=n_components)\n",
    "    feature_names, feature_columns = df_prep()\n",
    "    principal_components = pca_df.fit_transform(x)\n",
    "\n",
    "    column_names = []\n",
    "    for i in range (0, n_components):\n",
    "        count = str(i)\n",
    "        column_names.append('PC'+count)\n",
    "\n",
    "    column_names = np.array(column_names)\n",
    "    #components_DF = pd.dfFrame(df = principal_components, columns=column_names)\n",
    "    explained_var = pca_df.explained_variance_ratio_\n",
    "\n",
    "    explained_var_percentage = []\n",
    "    explained_percentage = 0\n",
    "    for i in range (len(explained_var)):\n",
    "        explained_percentage += (explained_var[i] * 100)   \n",
    "        explained_var_percentage.append(explained_var[i] *100)\n",
    "\n",
    "    explained_var_df = pd.dfFrame(explained_var_percentage)\n",
    "\n",
    "    explained_var_df.columns =[\"% of Information\"]\n",
    "    explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "    \n",
    "    return explained_percentage, n_components\n",
    "\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function_pca(n_components_fixed):\n",
    "    n_components = n_components_fixed\n",
    "    explained_percentage, n_components = pca(n_components)\n",
    "    information_component_ratio = (explained_percentage / n_components)\n",
    "    return {'loss': - information_component_ratio , 'status': STATUS_OK }\n",
    "\n",
    "loss = cost_function_pca(n_components_fixed)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
