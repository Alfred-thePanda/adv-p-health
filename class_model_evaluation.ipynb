{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- for final presentation, maybe mention development date for algorithms \n",
    "- visualisierung fÃ¼r probierte werte \n",
    "- vergleichbare projekte von anderen + deren reached accuracy ( zum vergleich, um zu beweisen, dass unsere gut?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0],axis = 1)\n",
    "\n",
    "# remove colums containing NaN values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "df.set_axis(range(len(df)), inplace=True)\n",
    "\n",
    "# removing target('diabetes') from features\n",
    "target = df['diabetes']\n",
    "features = df.drop(['diabetes'],axis=1)\n",
    "\n",
    "# splitting into training and test data\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(features_train.shape)\n",
    "# print(features_test.shape)\n",
    "# print(target_train.shape)\n",
    "# print(target_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Support Vector Machines**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy: 76 % with max_iter = 93\n",
    "\n",
    "-> Dont use, 2. & 3. perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing max_iter to reach the highest possible Accuracy\n",
    "\n",
    "# MAX_EVALS should be the same as/max the SEARCHSPACE so all possibilities are tried out\n",
    "# MAX_EVALS = 100\n",
    "# SEARCH_SPACE = [hp.randint('max_iter',100)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "# def cost_function(max_iter):\n",
    "#     max_iter = max_iter[0]\n",
    "#     print(max_iter)\n",
    "#     if max_iter == 0:\n",
    "#         return 0\n",
    "#     svm_classifier = make_pipeline(StandardScaler(), svm.SVC(max_iter=max_iter)).fit(features_train, target_train)\n",
    "#     svm_predictions = svm_classifier.predict(features_test)\n",
    "#    svm_accuracy = accuracy_score(target_test, svm_predictions)\n",
    "# \n",
    "#     return {'loss': - svm_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "# trials = Trials()\n",
    "# best = fmin(cost_function,\n",
    "#     space = SEARCH_SPACE,\n",
    "#     algo = tpe.suggest,\n",
    "#     max_evals = MAX_EVALS, \n",
    "#     trials = trials)\n",
    "\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for one specific 'person' #################################################################################################\n",
    "\n",
    "# max_iter = best['max_iter']\n",
    "# svm_classifier = make_pipeline(StandardScaler(), svm.SVC(max_iter=max_iter)).fit(features_train, target_train)\n",
    "\n",
    "# taking a random row from the test data to predict a result for:\n",
    "# def make_test_prediction(svm_classifier):\n",
    "#     rand_index = random.randint(0, 32581)\n",
    "#     test_row = features_test.iloc[rand_index] #.values.flatten().tolist()\n",
    "#     test_groundtruth = target_test.iloc[rand_index]\n",
    "#     prediction = svm_classifier.predict([test_row])\n",
    "#     \n",
    "#     return (prediction, test_groundtruth)\n",
    "\n",
    "# prediction, test_groundtruth = make_test_prediction(svm_classifier)\n",
    "# print (prediction == test_groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Stocastic Gradient Descent**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy:  88%, with max_iter = 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing max_iter to reach the highest possible Accuracy\n",
    "\n",
    "# MAX_EVALS should be the same as/max the SEARCHSPACE so all possibilities are tried out\n",
    "MAX_EVALS = 10\n",
    "SEARCH_SPACE = [hp.randint('max_iter',100)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "def cost_function(max_iter):\n",
    "    max_iter = max_iter[0]\n",
    "    if max_iter == 0:\n",
    "        return 0\n",
    "    sgd_classifier = make_pipeline(StandardScaler(), SGDClassifier(max_iter=max_iter)).fit(features_train, target_train)\n",
    "    sgd_predictions = sgd_classifier.predict(features_test)\n",
    "    sgd_accuracy = accuracy_score(target_test, sgd_predictions)\n",
    "\n",
    "    return {'loss': - sgd_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = best['max_iter']\n",
    "sgd_classifier = make_pipeline(StandardScaler(), SGDClassifier(max_iter=max_iter)).fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting for one specific 'person'  #################################################################################################\n",
    "\n",
    "# taking a random row from the test data to predict a result for:\n",
    "def make_test_prediction(sgd_classifier):\n",
    "    rand_index = random.randint(0, 32581)\n",
    "    test_row = features_test.iloc[rand_index]\n",
    "    test_groundtruth = target_test.iloc[rand_index]\n",
    "    prediction = sgd_classifier.predict([test_row])\n",
    "\n",
    "    return (prediction, test_groundtruth)\n",
    "\n",
    "prediction, test_groundtruth = make_test_prediction(sgd_classifier)\n",
    "print (prediction == test_groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Stocastic Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing n_estimators to reach the highest possible Accuracy\n",
    "\n",
    "# N_ESTIMATORS should be max the SEARCHSPACE so all possibilities are tried once\n",
    "N_ESTIMATORS = 10\n",
    "SEARCH_SPACE = [hp.randint('n_estimators',200)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "def cost_function(n_estimators):\n",
    "    n_estimators = n_estimators[0]\n",
    "    if n_estimators == 0:\n",
    "        return 0\n",
    "    sgb_classifier = make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=0.5, random_state=0)).fit(features_train, target_train)\n",
    "    sgb_predictions = sgb_classifier.predict(features_test)\n",
    "    sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "    return {'loss': - sgb_accuracy , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = N_ESTIMATORS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best loss: -0.8973674869175894\n",
    "\n",
    "{'n_estimators': 174}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = best['n_estimators']\n",
    "\n",
    "sgb_classifier = make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=0.5, random_state=0)).fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'model.pickle'\n",
    "\n",
    "pickle.dump(sgb_classifier, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgb_classifier = pickle.load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify performance / See false nagatives vs. false positives (ca: 7157 - 1855)\n",
    "\n",
    "predictions = sgb_classifier.predict(features_test)\n",
    "\n",
    "def calc_metrics(predictions, target_test):\n",
    "    prediction_true = false_negative = false_positive = 0\n",
    "\n",
    "    for i in range(0, 87331):\n",
    "        if target_test.iloc[i] == predictions[i]:\n",
    "            prediction_true+= 1\n",
    "        if (target_test.iloc[i] == 1 and predictions[i] == 0):\n",
    "            false_negative +=1\n",
    "        if (target_test.iloc[i] == 0 and predictions[i] == 1):\n",
    "            false_positive +=1\n",
    "    accuracy = 1 - (false_negative + false_positive) / len(predictions)\n",
    "\n",
    "    return (prediction_true, false_negative, false_positive, accuracy)\n",
    "\n",
    "prediction_true, false_negative, false_positive, accuracy = calc_metrics(predictions, target_test)\n",
    "print(prediction_true, false_negative, false_positive, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Predicting for one specific 'person' #################################################################################################\n",
    "\n",
    "# taking a random row from the test data to predict a result for:\n",
    "def make_test_prediction(sgb_classifier):\n",
    "    rand_index = random.randint(0, 32581)\n",
    "    test_row = features_test.iloc[rand_index] #.values.flatten().tolist()\n",
    "    test_groundtruth = target_test.iloc[rand_index]\n",
    "    prediction = sgb_classifier.predict([test_row])\n",
    "    \n",
    "    return (prediction, test_groundtruth)\n",
    "\n",
    "prediction, test_groundtruth = make_test_prediction(sgb_classifier)\n",
    "print (prediction)\n",
    "print (test_groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- simple SGB: Best reachable Accuracy:  87.33042784359463%, with max_iter= 74\n",
    "- Using a Pipepline/StandartScaler: Best reachable Accuracy:  100%, with max_iter = 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualisation\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette= \"Accent\")\n",
    "\n",
    "tids = [t['tid'] for t in trials.trials]\n",
    "n_estimators = [t['misc']['vals']['n_estimators']for t in trials]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tids, n_estimators)\n",
    "\n",
    "ax.legend(('n_estimators'), loc='lower right')\n",
    "ax.set_ylabel('n_estimators over all trials')\n",
    "ax.set_xlabel('trialIDs')\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "fig.savefig('./visualizations/tested_values.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
