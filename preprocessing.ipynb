{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Dataset preperation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing, cleaning, normalizint the data used. Clean dataset is exported as a .csv file afterwards, so this script only needs to be run once. The created file will be used in the principal component analysis.**\n",
    "\n",
    "\n",
    "Taget Variable: DIABETE4: (Have you ever been told you had diabetes?)\n",
    "Values: \n",
    "- 1: Yes \n",
    "- 2: only during pregnancy\n",
    "- 3: No\n",
    "- 4: prediabetes/borderline diabetes (so diabetes-risk?)\n",
    "- 7: Dont know (how tf dont u know?\n",
    "- 9: refused answer)\n",
    "- BLANK: missing/not asked\n",
    "\n",
    "\n",
    "drop: BLANKS, 9, 7 \n",
    "combine: 2 & 3 as 'no'\n",
    "keep 4 ( classify as risk)\n",
    "\n",
    "After cleaning: \n",
    "- 0: No\n",
    "- 1: Yes\n",
    "- 2: Is/was at risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sas('./data/LLCP2021.XPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################### Dataset preperation ###\n",
    "\n",
    "# removing 7, 9, NaN from data\n",
    "data.drop(data.loc[data[\"DIABETE4\"]==7].index, inplace = True)\n",
    "data.drop(data.loc[data[\"DIABETE4\"]==9].index, inplace = True)\n",
    "data = data.dropna(subset=[\"DIABETE4\"])\n",
    "pd.unique(data[\"DIABETE4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(data, columns = data.columns)\n",
    "\n",
    "# remove colums containing string values (TODO: if time change some of those later to 1-0 etc...)\n",
    "cleaned_dataframe_usable = pca_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DIABETE4 as target var, remove from input variables\n",
    "target = pd.DataFrame(pca_df['DIABETE4'])\n",
    "features = cleaned_dataframe_usable.drop(['DIABETE4'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine answers by changing \"3\" (no) answers to \"0\"(no) \n",
    "# combine answers by changing \"2\" (during pregnancy) answers to \"0\"(no) \n",
    "# combine answers by changing \"4\" (at risk) answers to \"0\"(no) \n",
    "# TODO: include at risk later on!\n",
    "#target.replace(3.0, 0.0, inplace=True)\n",
    "#target.replace(2.0, 0.0, inplace=True)\n",
    "#target.replace(4.0, 0.0, inplace=True)\n",
    "\n",
    "pca_df.replace(3.0, 0.0, inplace=True)\n",
    "pca_df.replace(2.0, 0.0, inplace=True)\n",
    "pca_df.replace(4.0, 0.0, inplace=True)\n",
    "\n",
    "# check if target only consists of 3 values now, o-No, 1-Yes, 2-at risk\n",
    "# print(pd.unique(target[\"DIABETE4\"]))\n",
    "# print(pd.unique(data[\"DIABETE4\"]))\n",
    "\n",
    "# save feature names in a variable\n",
    "feature_names = list(features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"No. of columns containing null values\")\n",
    "#print(len(cleaned_dataframe_usable.columns[ cleaned_dataframe_usable.isna().any()]))\n",
    "\n",
    "#print(\"No. of columns not containing null values\")\n",
    "#print(len(  cleaned_dataframe_usable.columns[cleaned_dataframe_usable.notna().all()]))\n",
    "\n",
    "#print(\"Total no. of columns in the dataframe\")\n",
    "#print(len(cleaned_dataframe_usable.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_dataframe_usable.to_csv(r'.\\data\\clean_preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standartizing and normalizing data\n",
    "x = cleaned_dataframe_usable.loc[:,feature_names].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "# np.mean(x), np.std(x)\n",
    "\n",
    "## Format back into dataframe\n",
    "feature_columns = ['feature_names' + str(i) for i in range (x.shape[1])]\n",
    "normalized_df = pd.DataFrame(x, columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to wich number of features should be reduced? (max 79)\n",
    "n_components = 5 \n",
    "\n",
    "# actual PCA\n",
    "pca_df = PCA(n_components=n_components)\n",
    "principal_components = pca_df.fit_transform(x)\n",
    "\n",
    "column_names = []\n",
    "for i in range (0, n_components):\n",
    "    count = str(i)\n",
    "    column_names.append('PC'+count)\n",
    "\n",
    "column_names = np.array(column_names)\n",
    "print(column_names)\n",
    "\n",
    "components_DF = pd.DataFrame(data = principal_components, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with percentage each component adds (output is sorted)\n",
    "explained_var = pca_df.explained_variance_ratio_\n",
    "\n",
    "explained_var_per = []\n",
    "for i in range (len(explained_var)):    \n",
    "    explained_var_per.append(explained_var[i] *100)\n",
    "\n",
    "explained_var_df = pd.DataFrame(explained_var_per)\n",
    "\n",
    "explained_var_df.columns =[\"% of Information\"]\n",
    "explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "explained_var_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
