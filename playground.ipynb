{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "px_template = \"simple white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/DIABETE4_health_indicators_BRFSS2015.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So basically is erm KNN & Decision tree zoid, bzw. daccuracy ned hoch gnua\n",
    "imma test naive bayes, and cnn? \n",
    "am besten in da final üresentation dann erwähnen, wann classifiers entwickelt wurdn**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0],axis = 1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIABETE4', ['diabetes']]\n"
     ]
    }
   ],
   "source": [
    "# search for columns containing 'DIABETE' to find target var\n",
    "\n",
    "diabate_cols = [col for col in df.columns if 'DIABETE' in col]\n",
    "diabate_cols.append([col for col in df.columns if 'diabete' in col])\n",
    "print(diabate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of columns containing null values\n",
      "0\n",
      "No. of columns not containing null values\n",
      "176\n",
      "Total no. of columns in the dataframe\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "# remove colums containing string values\n",
    "df = df.dropna(axis=1)\n",
    "df.shape\n",
    "\n",
    "print(\"No. of columns containing null values\")\n",
    "print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "print(\"No. of columns not containing null values\")\n",
    "print(len(df.columns[df.notna().all()]))\n",
    "\n",
    "print(\"Total no. of columns in the dataframe\")\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing target var from feature list\n",
    "target = df['diabetes']\n",
    "# target = target.to_frame()\n",
    "features = df.drop(['diabetes'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and test data\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349323, 175)\n",
      "(87331, 175)\n",
      "(349323,)\n",
      "(87331,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a model\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree vizualisation\n",
    "# target_names = [\"diabetes\", \"no-diabetes\"]\n",
    "\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = tree.plot_tree(model, \n",
    "#                     feature_names=features_train.columns,  \n",
    "#                     class_names=target_names,\n",
    "#                     filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87331,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.88623741855699"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_accuracy = accuracy_score(target_test, predictions)\n",
    "tree_acc_percentage = tree_accuracy * 100\n",
    "tree_acc_percentage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=15)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=15)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "knn_model = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "knn_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions = knn_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.63475741718291"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_accuracy = accuracy_score(target_test, knn_predictions)\n",
    "knn_acc_percentage = knn_accuracy * 100\n",
    "knn_acc_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 84.88623741855699\n",
      "KNN Accuracy: 86.63475741718291\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Accuracy: \" + str(tree_acc_percentage))\n",
    "print(\"KNN Accuracy: \" + str(knn_acc_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a cnn \n",
    "# model = MLPClassifier(hidden_layer_sizes=(50,20, 10), random_state=1,\n",
    "#               solver='adam')\n",
    "# model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing dimensionality \n",
    "# print(target_train.shape)\n",
    "# print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting for test data\n",
    "# predictions = model.predict([features_test])\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.Hyperopt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df/DIABETE4_health_indicators_BRFSS2015.csv')\n",
    "features = df.drop('DIABETE4',axis=1)\n",
    "groundtruth = df['DIABETE4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable DIABETE4:\n",
    "#- 0 : no diabetes/ only during pregnancy\n",
    "#- 1 : prediabetes\n",
    "#- 2 : diabetes\n",
    "\n",
    "### Classifier: #######################################################################################################\n",
    "\n",
    "def no_diabetes_classifier(groundtruth) -> bool:\n",
    "    if (groundtruth == 0):\n",
    "        return (True)\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def diabetes_classifier(groundtruth) -> bool:     \n",
    "    if (groundtruth == 2.0):\n",
    "        return(True) \n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def classifier(features):\n",
    "    predictions = []\n",
    "    for i in range(0, len(features)):\n",
    "        # is it diabetes?\n",
    "        if ((diabetes_classifier(features[i]))):\n",
    "            predictions.append(2.0)\n",
    "        # is it no diabetes?\n",
    "        elif (no_diabetes_classifier(features[i])):\n",
    "            predictions.append(0.0)\n",
    "        # its neither, so patient is at risk\n",
    "        else: predictions.append(1.0)\n",
    "    # print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define performance metrics (True negatives, false negatives, true positives, false positives, recall, specificity, precision)\n",
    "    # recall = Sensitifity = True Positive Rate\n",
    "    # specificity = True Negative Rate\n",
    "    # precision = TP / (TP + FP)\n",
    "\n",
    "def calc_metrics(predictions, groundtruth):\n",
    "    # predictions = predictions\n",
    "    TP_no_diabetes = TN_prediabetes = TN_diabetes = TN_no_diabetes = TN_diabetes = TP_prediabetes = TP_diabetes = FP_no_diabetes = FN_diabetes = FN_prediabetes = FP_diabetes = FP_prediabetes = FN_no_diabetes = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if (predictions[i] == groundtruth[i]):\n",
    "            if (predictions[i] == 0):\n",
    "                TP_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif (predictions[i] ==1):\n",
    "                TP_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "            else : \n",
    "                TP_diabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                \n",
    "        elif (predictions[i] == 0):\n",
    "            if (groundtruth[i] == 1):\n",
    "                FP_no_diabetes += 1\n",
    "                FN_prediabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif (groundtruth[i] == 2):\n",
    "                FP_no_diabetes += 1\n",
    "                FN_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "                \n",
    "        elif (predictions[i] == 1):\n",
    "            if (groundtruth[i] == 0):\n",
    "                FP_prediabetes += 1\n",
    "                FN_no_diabetes += 1\n",
    "                TN_diabetes += 1\n",
    "            elif(groundtruth[i] == 2):\n",
    "                FP_prediabetes += 1\n",
    "                FN_diabetes += 1\n",
    "                TN_no_diabetes+= 1\n",
    "                \n",
    "        elif (predictions[i] == 2):\n",
    "            if (groundtruth[i] == 0):\n",
    "                FP_diabetes += 1\n",
    "                FN_no_diabetes += 1\n",
    "                TN_prediabetes += 1\n",
    "            elif(groundtruth[i] == 1):\n",
    "                FP_diabetes += 1\n",
    "                FN_prediabetes += 1\n",
    "                TN_no_diabetes += 1\n",
    "                \n",
    "    TP_total = TP_diabetes + TP_no_diabetes + TP_prediabetes \n",
    "    TN_total = TN_diabetes + TN_no_diabetes + TN_prediabetes\n",
    "    FP_total = FP_diabetes + FP_no_diabetes + FP_prediabetes\n",
    "    FN_total = FN_diabetes + FN_no_diabetes + FN_prediabetes \n",
    "    recall = TP_total/ (TP_total + FN_total)\n",
    "    specificity =  TN_total / (TN_total + FP_total)\n",
    "    precision = (TP_total / (TP_total + FP_total))\n",
    "    return (recall, specificity, precision)\n",
    "\n",
    "\n",
    "recall, specificity, precision = calc_metrics(predictions, groundtruth)\n",
    "print (recall, specificity, precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt for number of optimal components with most information/ number ob components\n",
    "\n",
    "### Constants ##############################################################################################################\n",
    "\n",
    "# amount of trials\n",
    "MAX_EVALS = 3\n",
    "\n",
    "# One 'run' equals one fmin-execution where each run for a number of x trials\n",
    "SEARCH_SPACE = [hp.uniform('default', 0, 1)]\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function(features):\n",
    "    predictions = classifier(groundtruth)\n",
    "    recall, specifity, precision = calc_metrics(predictions, groundtruth)\n",
    "    # print(f\"recall: {recall}\")\n",
    "    # print(f\"specificity: {specifity}\")\n",
    "    # specifity is maximized (= minimizes not recognized diabetes cases)\n",
    "    print(f\"Specifity (TN-Rate): {specifity}\")\n",
    "    return {'loss': - specifity , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Optimal Component configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df/DIABETE4_health_indicators_BRFSS2015.csv')\n",
    "features = df.drop('DIABETE4',axis=1)\n",
    "groundtruth = df['DIABETE4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standartizing and normalizing df\n",
    "feature_names = list(features.columns)\n",
    "\n",
    "x = df.loc[:,feature_names].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "# np.mean(x), np.std(x)\n",
    "\n",
    "## Format back into dfframe\n",
    "feature_columns = ['feature_names' + str(i) for i in range (x.shape[1])]\n",
    "normalized_df = pd.dfFrame(x, columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to wich number of features should be reduced? (max 79)\n",
    "n_components = 5 \n",
    "\n",
    "# actual PCA\n",
    "pca_df = PCA(n_components=n_components)\n",
    "principal_components = pca_df.fit_transform(x)\n",
    "\n",
    "column_names = []\n",
    "for i in range (0, n_components):\n",
    "    count = str(i)\n",
    "    column_names.append('PC'+count)\n",
    "\n",
    "column_names = np.array(column_names)\n",
    "print(column_names)\n",
    "\n",
    "components_DF = pd.dfFrame(df = principal_components, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dfframe with percentage each component adds (output is sorted)\n",
    "explained_var = pca_df.explained_variance_ratio_\n",
    "explained_percentage = 0\n",
    "\n",
    "explained_var_per = []\n",
    "for i in range (len(explained_var)):\n",
    "    explained_percentage += (explained_var[i] * 100)   \n",
    "    explained_var_per.append(explained_var[i] *100)\n",
    "\n",
    "explained_var_df = pd.dfFrame(explained_var_per)\n",
    "\n",
    "explained_var_df.columns =[\"% of Information\"]\n",
    "explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "# explained_var_df\n",
    "explained_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ##############################################################################################################\n",
    "\n",
    "# amount of trials\n",
    "MAX_EVALS = 2\n",
    "SEARCH_SPACE = [hp.uniform('number_of_components',0,20)]\n",
    "\n",
    "### df preperation ##############################################################################################################\n",
    "\n",
    "# Standartizing and normalizing df\n",
    "def df_prep(df):\n",
    "    feature_names = list(features.columns)\n",
    "    x = df.loc[:,feature_names].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    return x\n",
    "\n",
    "# PCA\n",
    "def pca (number_of_components):\n",
    "    pca_df = PCA(n_components=number_of_components)\n",
    "    explained_percentage = 0\n",
    "    explained_var_percentages = []\n",
    "    column_names = []\n",
    "    x = df_prep(df)\n",
    "    principal_components = pca_df.fit_transform(x)\n",
    "    \n",
    "    for i in range (0, number_of_components):\n",
    "        count = str(i)\n",
    "        column_names.append('PC'+count)\n",
    "\n",
    "    column_names = np.array(column_names)\n",
    "    explained_var = pca_df.explained_variance_ratio_\n",
    "\n",
    "    for i in range (len(explained_var)):\n",
    "        explained_percentage += (explained_var[i] * 100)   \n",
    "        explained_var_percentages.append(explained_var[i] *100)\n",
    "\n",
    "    explained_var_df = pd.dfFrame(explained_var_percentages)\n",
    "    explained_var_df.columns =[\"% of Information\"]\n",
    "    explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "    \n",
    "    return explained_percentage, number_of_components\n",
    "\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function_pca(number_of_components):\n",
    "    number_of_components = int(number_of_components[0])\n",
    "    #print(number_of_components)\n",
    "    explained_percentage, number_of_components = pca(number_of_components)\n",
    "    information_component_ratio = (explained_percentage / number_of_components)\n",
    "    return {'loss': - information_component_ratio , 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function_pca,\n",
    "    space = SEARCH_SPACE,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = MAX_EVALS, \n",
    "    trials = trials)\n",
    "\n",
    "def cleanup_loss():\n",
    "    raw_loss = float(trials.best_trial['result']['loss'])\n",
    "    #raw_loss.dtype\n",
    "    print(raw_loss)\n",
    "    n = int(trials.best_trial['misc']['vals']['number_of_components'])\n",
    "    print (n)\n",
    "    clean__reached_percentage = raw_loss * n\n",
    "\n",
    "    return clean__reached_percentage\n",
    "\n",
    "\n",
    "\n",
    "perc = cleanup_loss()\n",
    "print(trials.best_trial)\n",
    "print(\"percentage for {n} coponents: {perc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ##############################################################################################################\n",
    "MAX_EVALS = 3\n",
    "n_components_fixed = 10\n",
    "\n",
    "### df preperation ##############################################################################################################\n",
    "\n",
    "# Standartizing and normalizing df\n",
    "\n",
    "def df_prep():\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    x = df.loc[:,feature_names].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    feature_columns = ['feature_names' + str(i) for i in range (x.shape[1])]\n",
    "    normalized_df = pd.dfFrame(x, columns=feature_columns)\n",
    "\n",
    "    return feature_names, feature_columns\n",
    "\n",
    "\n",
    "# actual PCA\n",
    "\n",
    "def pca (n_components):\n",
    "    pca_df = PCA(n_components=n_components)\n",
    "    feature_names, feature_columns = df_prep()\n",
    "    principal_components = pca_df.fit_transform(x)\n",
    "\n",
    "    column_names = []\n",
    "    for i in range (0, n_components):\n",
    "        count = str(i)\n",
    "        column_names.append('PC'+count)\n",
    "\n",
    "    column_names = np.array(column_names)\n",
    "    #components_DF = pd.dfFrame(df = principal_components, columns=column_names)\n",
    "    explained_var = pca_df.explained_variance_ratio_\n",
    "\n",
    "    explained_var_percentage = []\n",
    "    explained_percentage = 0\n",
    "    for i in range (len(explained_var)):\n",
    "        explained_percentage += (explained_var[i] * 100)   \n",
    "        explained_var_percentage.append(explained_var[i] *100)\n",
    "\n",
    "    explained_var_df = pd.dfFrame(explained_var_percentage)\n",
    "\n",
    "    explained_var_df.columns =[\"% of Information\"]\n",
    "    explained_var_df[\"feature_nr\"] = column_names.tolist()\n",
    "    \n",
    "    return explained_percentage, n_components\n",
    "\n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "def cost_function_pca(n_components_fixed):\n",
    "    n_components = n_components_fixed\n",
    "    explained_percentage, n_components = pca(n_components)\n",
    "    information_component_ratio = (explained_percentage / n_components)\n",
    "    return {'loss': - information_component_ratio , 'status': STATUS_OK }\n",
    "\n",
    "loss = cost_function_pca(n_components_fixed)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
