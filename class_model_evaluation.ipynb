{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- for final presentation, maybe mention development date for algorithms \n",
    "- visualisierung für probierte werte \n",
    "- vergleichbare projekte von anderen + deren reached accuracy ( zum vergleich, um zu beweisen, dass unsere gut?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/brfss_imputed.csv')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# remove double indexing\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# # checking for nan's\n",
    "# print(\"No. of columns containing null values\")\n",
    "# print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "# print(\"No. of rows containing null values\")\n",
    "# print(df.isnull().any(axis=1).sum())\n",
    "\n",
    "# dropping unsused columns\n",
    "df = df[df.columns.drop(list(df.filter(regex='unk_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='not_known_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='_was_missing')))]\n",
    "\n",
    "\n",
    "# splitting into target & features df\n",
    "target = df['DIABETE4']\n",
    "features = df.drop(['DIABETE4'], axis=1)\n",
    "\n",
    "# splitting into training and test data\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(features_train.shape)\n",
    "# print(features_test.shape)\n",
    "# print(target_train.shape)\n",
    "# print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. XG - Boosting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Optimizing n_estimators & max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [1:19:56<3:18:02, 349.48s/trial, best loss: -0.8680268868423838]"
     ]
    }
   ],
   "source": [
    "# Optimizing n_estimators to reach the highest possible accuracy\n",
    "\n",
    "# N_ESTIMATORS should be max the SEARCHSPACE so all possibilities are tried once\n",
    "MAX_EVALS = 50\n",
    "SEARCH_SPACE = [hp.uniformint('n_estimators', 3000, 30000), hp.uniformint('max_depth', 0, 100)]   \n",
    "\n",
    "### Optimizaion ##############################################################################################################\n",
    "\n",
    "\n",
    "def cost_function(params):\n",
    "    n_estimators = params[0]\n",
    "    max_depth = params[1]\n",
    "    if n_estimators == 0:\n",
    "        return 0\n",
    "    xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=max_depth, random_state = 0).fit(features_train, target_train)\n",
    "    sgb_predictions = xgb_classifier.predict(features_test)\n",
    "    sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "    return {'loss': - sgb_accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(cost_function,\n",
    "            space=SEARCH_SPACE,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=MAX_EVALS,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best loss:  -0.871387742140362\n",
    "\n",
    "{'n_estimators': 2265.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Training & saving model with optimized n_estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = int(best['n_estimators'])\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3).fit(features_train, target_train)\n",
    "sgb_predictions = xgb_classifier.predict(features_test)\n",
    "sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "\n",
    "\n",
    "filename = 'xgb_model.pickle'\n",
    "\n",
    "pickle.dump(xgb_classifier, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_final.pickle'\n",
    "\n",
    "pickle.dump(xgb_classifier, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Importing model to predict for one person**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example' #################################################################################################\n",
    "\n",
    "xgb_classifier = pickle.load(open('model_final.pickle', \"rb\"))\n",
    "\n",
    "\n",
    "# This would be the answers given by the user \n",
    "# (here i am getting a random row from the dataset for demonstration)\n",
    "example_input = features_test.iloc[576] \n",
    "# print(example_input)\n",
    "\n",
    "\n",
    "def make_prediction (xgb_classifier, input):\n",
    "    prediction = xgb_classifier.predict(input)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "prediction = make_prediction(xgb_classifier, example_input)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Vizualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested values over all trials\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"Accent\")\n",
    "\n",
    "tids = [t['tid'] for t in trials.trials]\n",
    "n_estimators = [t['misc']['vals']['n_estimators']for t in trials]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tids, n_estimators)\n",
    "\n",
    "ax.legend(('n_estimators'), loc='lower right')\n",
    "ax.set_ylabel('n_estimators over all trials')\n",
    "ax.set_xlabel('trialIDs')\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "fig.savefig('./visualizations/tested_values.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss Improvement over Trials\n",
    "from itertools import chain\n",
    "\n",
    "# loss Improvement over Trials\n",
    "\n",
    "def plot_reached_min_losses(trials):\n",
    "    losses = [t['result']['loss'] for t in trials]\n",
    "    tids = [t['tid'] for t in trials.trials]\n",
    "    n_estimators = [t['misc']['vals']['n_estimators'] for t in trials]\n",
    "\n",
    "    n_estimators = list(chain.from_iterable(n_estimators))\n",
    "\n",
    "    best_loss = losses[0]\n",
    "    points_to_plot = []\n",
    "    points_to_plot.append(losses[0])\n",
    "    tids_with_loss_improvement = [0]\n",
    "    counter = 0\n",
    "    for i in range(1, len(losses)):\n",
    "        if losses[i] < best_loss:\n",
    "            tid = tids[i]\n",
    "            # print(tid)\n",
    "            points_to_plot.append(losses[i])\n",
    "            tids_with_loss_improvement.append(tid)\n",
    "            best_loss = losses[i]\n",
    "\n",
    "    # plotting with logarithmic y-scale\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots()\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_ylabel('developement of min loss')\n",
    "    ax.set_xlabel('Trial-IDs')\n",
    "    fig.set_size_inches(15, 5)\n",
    "    ax.plot(tids_with_loss_improvement, points_to_plot,\n",
    "            color=\"mediumpurple\", linestyle='-', marker='o')\n",
    "    ax.scatter(tids, losses, color='skyblue')\n",
    "\n",
    "\n",
    "plot_reached_min_losses(trials)\n",
    "\n",
    "fig.savefig('./visualizations/loss_improvement.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
