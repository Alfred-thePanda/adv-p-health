{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- for final presentation, maybe mention development date for algorithms \n",
    "- visualisierung fÃ¼r probierte werte \n",
    "- vergleichbare projekte von anderen + deren reached accuracy ( zum vergleich, um zu beweisen, dass unsere gut?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302304, 81)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/brfss_imputed.csv')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# remove double indexing\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# # checking for nan's\n",
    "# print(\"No. of columns containing null values\")\n",
    "# print(len(df.columns[df.isna().any()]))\n",
    "\n",
    "# print(\"No. of rows containing null values\")\n",
    "# print(df.isnull().any(axis=1).sum())\n",
    "\n",
    "# dropping unsused columns\n",
    "df = df[df.columns.drop(list(df.filter(regex='unk_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='not_known_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='_was_missing')))]\n",
    "df = df[df.columns.drop([\"CVDCRHD4\", \"CVDINFR4\", \"CVDSTRK3\", \"ASTHMA3\", \"CHCOCNCR\",\n",
    "                        \"ASTHNOW\", \"CHCSCNCR\", \"CHCCOPD3\", \"ADDEPEV3\", \"CHCKDNY2\", \"HAVARTH5\"])]\n",
    "\n",
    "# splitting into target & features df\n",
    "target = df['DIABETE4']\n",
    "features = df.drop(['DIABETE4'], axis=1)\n",
    "\n",
    "# splitting into training and test data\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "print(features_train.shape)\n",
    "# print(features_test.shape)\n",
    "# print(target_train.shape)\n",
    "# print(target_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Optimizing n_estimators & max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimizing n_estimators to reach the highest possible accuracy\n",
    "\n",
    "# # N_ESTIMATORS should be max the SEARCHSPACE so all possibilities are tried once\n",
    "# MAX_EVALS =\n",
    "# SEARCH_SPACE = [hp.uniformint('n_estimators', 3000, 30000), hp.uniformint('max_depth', 0, 100)]\n",
    "\n",
    "# ### Optimizaion ##############################################################################################################\n",
    "\n",
    "\n",
    "# def cost_function(params):\n",
    "#     n_estimators = params[0]\n",
    "#     max_depth = params[1]\n",
    "#     if n_estimators == 0:\n",
    "#         return 0\n",
    "#     xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=max_depth, random_state = 0).fit(features_train, target_train)\n",
    "#     sgb_predictions = xgb_classifier.predict(features_test)\n",
    "#     sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "#     return {'loss': - sgb_accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# trials = Trials()\n",
    "# best = fmin(cost_function,\n",
    "#             space=SEARCH_SPACE,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=MAX_EVALS,\n",
    "#             trials=trials)\n",
    "\n",
    "# print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best loss:  -0.871387742140362\n",
    "\n",
    "{'n_estimators': 2265.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Training & saving model with optimized n_estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 226\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic',\n",
    "                                   tree_method='hist', eta=0.1, max_depth=3).fit(features_train, target_train)\n",
    "sgb_predictions = xgb_classifier.predict(features_test)\n",
    "sgb_accuracy = accuracy_score(target_test, sgb_predictions)\n",
    "\n",
    "\n",
    "filename = 'xgb_model.pickle'\n",
    "\n",
    "pickle.dump(xgb_classifier, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680136551286123\n"
     ]
    }
   ],
   "source": [
    "print(sgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_final.pickle'\n",
    "\n",
    "pickle.dump(xgb_classifier, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Importing model to predict for one person**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81,)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[10:24:50] C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/predictor/cpu_predictor.cc:377: Check failed: m->NumColumns() == model.learner_model_param->num_feature (1 vs. 81) : Number of columns in data must equal to trained model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m     prediction \u001b[39m=\u001b[39m xgb_classifier\u001b[39m.\u001b[39mpredict(\u001b[39minput\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction\n\u001b[1;32m---> 18\u001b[0m prediction \u001b[39m=\u001b[39m make_prediction(xgb_classifier, example_input)\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(prediction)\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mmake_prediction\u001b[1;34m(xgb_classifier, input)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_prediction\u001b[39m(xgb_classifier, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     prediction \u001b[39m=\u001b[39m xgb_classifier\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32mc:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1525\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1517\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1524\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m-> 1525\u001b[0m         class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1526\u001b[0m             X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1527\u001b[0m             output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[0;32m   1528\u001b[0m             ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[0;32m   1529\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1530\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1531\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1532\u001b[0m         )\n\u001b[0;32m   1533\u001b[0m         \u001b[39mif\u001b[39;00m output_margin:\n\u001b[0;32m   1534\u001b[0m             \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m             \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\sklearn.py:1114\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1114\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1115\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1116\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1117\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1118\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1119\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1120\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1121\u001b[0m         )\n\u001b[0;32m   1122\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1123\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:2291\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _ensure_np_dtype\n\u001b[0;32m   2290\u001b[0m     data, _ \u001b[39m=\u001b[39m _ensure_np_dtype(data, data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m-> 2291\u001b[0m     _check_call(\n\u001b[0;32m   2292\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterPredictFromDense(\n\u001b[0;32m   2293\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   2294\u001b[0m             _array_interface(data),\n\u001b[0;32m   2295\u001b[0m             from_pystr_to_cstr(json\u001b[39m.\u001b[39;49mdumps(args)),\n\u001b[0;32m   2296\u001b[0m             p_handle,\n\u001b[0;32m   2297\u001b[0m             ctypes\u001b[39m.\u001b[39;49mbyref(shape),\n\u001b[0;32m   2298\u001b[0m             ctypes\u001b[39m.\u001b[39;49mbyref(dims),\n\u001b[0;32m   2299\u001b[0m             ctypes\u001b[39m.\u001b[39;49mbyref(preds),\n\u001b[0;32m   2300\u001b[0m         )\n\u001b[0;32m   2301\u001b[0m     )\n\u001b[0;32m   2302\u001b[0m     \u001b[39mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcsr_matrix):\n",
      "File \u001b[1;32mc:\\Users\\carme\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [10:24:50] C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-08de971ced8a8cdc6-1/xgboost/xgboost-ci-windows/src/predictor/cpu_predictor.cc:377: Check failed: m->NumColumns() == model.learner_model_param->num_feature (1 vs. 81) : Number of columns in data must equal to trained model."
     ]
    }
   ],
   "source": [
    "# Usage Example' #################################################################################################\n",
    "\n",
    "xgb_classifier = pickle.load(open('model_final.pickle', \"rb\"))\n",
    "\n",
    "\n",
    "# This would be the answers given by the user\n",
    "# (here i am getting a random row from the dataset for demonstration)\n",
    "example_input = features_test.iloc[576]\n",
    "example_input = example_input.to_numpy()\n",
    "print ( example_input.shape)\n",
    "\n",
    "def make_prediction(xgb_classifier, input):\n",
    "    prediction = xgb_classifier.predict(input)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "prediction = make_prediction(xgb_classifier, example_input)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Vizualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested values over all trials\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"Accent\")\n",
    "\n",
    "tids = [t['tid'] for t in trials.trials]\n",
    "n_estimators = [t['misc']['vals']['n_estimators']for t in trials]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tids, n_estimators)\n",
    "\n",
    "ax.legend(('n_estimators'), loc='lower right')\n",
    "ax.set_ylabel('n_estimators over all trials')\n",
    "ax.set_xlabel('trialIDs')\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "fig.savefig('./visualizations/tested_values.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss Improvement over Trials\n",
    "from itertools import chain\n",
    "\n",
    "# loss Improvement over Trials\n",
    "\n",
    "\n",
    "def plot_reached_min_losses(trials):\n",
    "    losses = [t['result']['loss'] for t in trials]\n",
    "    tids = [t['tid'] for t in trials.trials]\n",
    "    n_estimators = [t['misc']['vals']['n_estimators'] for t in trials]\n",
    "\n",
    "    n_estimators = list(chain.from_iterable(n_estimators))\n",
    "\n",
    "    best_loss = losses[0]\n",
    "    points_to_plot = []\n",
    "    points_to_plot.append(losses[0])\n",
    "    tids_with_loss_improvement = [0]\n",
    "    counter = 0\n",
    "    for i in range(1, len(losses)):\n",
    "        if losses[i] < best_loss:\n",
    "            tid = tids[i]\n",
    "            # print(tid)\n",
    "            points_to_plot.append(losses[i])\n",
    "            tids_with_loss_improvement.append(tid)\n",
    "            best_loss = losses[i]\n",
    "\n",
    "    # plotting with logarithmic y-scale\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots()\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_ylabel('developement of min loss')\n",
    "    ax.set_xlabel('Trial-IDs')\n",
    "    fig.set_size_inches(15, 5)\n",
    "    ax.plot(tids_with_loss_improvement, points_to_plot,\n",
    "            color=\"mediumpurple\", linestyle='-', marker='o')\n",
    "    ax.scatter(tids, losses, color='skyblue')\n",
    "\n",
    "\n",
    "plot_reached_min_losses(trials)\n",
    "\n",
    "fig.savefig('./visualizations/loss_improvement.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
