{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- for final presentation, maybe mention development date for algorithms \n",
    "- visualisierung f√ºr probierte werte \n",
    "- vergleichbare projekte von anderen + deren reached accuracy ( zum vergleich, um zu beweisen, dass unsere gut?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/brfss_imputed.csv')\n",
    "\n",
    "# remove double indexing\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# dropping unsused columns\n",
    "df = df[df.columns.drop(list(df.filter(regex='unk_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='not_known_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='_was_missing')))]\n",
    "df = df[df.columns.drop([\"CVDCRHD4\", \"CVDINFR4\", \"CVDSTRK3\", \"ASTHMA3\", \"CHCOCNCR\",\n",
    "                        \"ASTHNOW\", \"CHCSCNCR\", \"CHCCOPD3\", \"ADDEPEV3\", \"CHCKDNY2\", \"HAVARTH5\"])]\n",
    "\n",
    "# splitting into target & features df\n",
    "target = df['DIABETE4']\n",
    "features = df.drop(['DIABETE4'], axis=1)\n",
    "\n",
    "categoric_features = ['SEXVAR', 'GENHLTH', 'PERSDOC3', 'MEDCOST1', 'CHECKUP1', 'EXERANY2', 'BPMEDS', 'TOLDHI3', 'CHOLMED3', 'DIABETE4', 'VETERAN3', 'PREGNANT', 'DEAF', 'BLIND', 'DECIDE', 'DIFFWALK',\n",
    "                      'DIFFDRES', 'DIFFALON', 'SMOKE100', 'FLUSHOT7', 'PNEUVAC4', 'HIVTST7', '_METSTAT', '_URBSTAT', '_HLTHPLN', '_INCOMG1', 'high_blood_pressure', 'pregnant_high_blood_pressure',\n",
    "                      'borderline_high_blood_pressure', 'cholesterol_checked_within_year', 'married', 'divorced', 'widowed', 'separated', 'never_married', 'unmarried_couple', 'own_house',\n",
    "                      'renting', 'other_arrangement_housing', 'employed_for_wages', 'self_employed', 'out_of_work_year_plus', 'out_of_work_year_less', 'homemaker', 'student', 'retired', 'unable_to_work',\n",
    "                      'smoke_every_day', 'smoke_some_days', 'smoke_not_at_all', 'smokeless_every_day', 'smokeless_some_days', 'smokeless_not_at_all', 'ecig_every_day', 'ecig_some_days', 'ecig_not_at_all',\n",
    "                      'ecig_never_used', 'white', 'black', 'asian', 'native', 'hispanic', 'not_graduate_high_school', 'graduated_high_school', 'attended_college', 'graduated_college', 'DIABETE4']\n",
    "\n",
    "for cat in categoric_features:\n",
    "    df[cat].astype(\"category\")\n",
    "\n",
    "\n",
    "# splitting into training and test data\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **xgboost Classification**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sckikit-learn Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_range = np.arange(0.01, 1, 0.05)\n",
    "max_depth = np.arange(1, 5, 1)\n",
    "test_XG = [] \n",
    "train_XG = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "for lr in learning_rate_range:\n",
    "    for i in max_depth:\n",
    "        xgb_classifier = xgb.XGBClassifier(eta = lr, tree_method=\"gpu_hist\", max_depth = i,  enable_categorical=True)\n",
    "        xgb_classifier.fit(features_train, target_train)\n",
    "\n",
    "        # predictions_train = xgb_classifier.predict(features_train)\n",
    "        # predictions_test = xgb_classifier.predict(features_test)\n",
    "\n",
    "        # train_accuracy.append(accuracy_score(target_train, predictions_train))\n",
    "        # test_accuracy.append(accuracy_score(target_test, predictions_test))\n",
    "\n",
    "        train_XG.append(xgb_classifier.score(features_train, target_train))\n",
    "        test_XG.append(xgb_classifier.score(features_test, target_test))\n",
    "\n",
    "# print(xgb_classifier.feature_importances_)\n",
    "\n",
    "# print(max(train_accuracy))\n",
    "# print(max(test_accuracy))\n",
    "print(max(train_XG))\n",
    "print(max(test_XG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.Booster()\n",
    "xgb_classifier.load_model(\"xgb_model.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(learning_rate_range, train_XG, c='orange', label='Train')\n",
    "plt.plot(learning_rate_range, test_XG, c='m', label='Test')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.xticks(learning_rate_range)\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.ylim(0.6, 1)\n",
    "plt.legend(prop={'size': 12}, loc=3)\n",
    "plt.title('Accuracy score vs. Learning rate of XGBoost', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **xgboost** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to prepare data as DMatrix objects\n",
    "train = xgb.DMatrix(features_train, target_train)\n",
    "test = xgb.DMatrix(features_test, target_test)\n",
    "\n",
    "# We need to define parameters as dict\n",
    "params = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    # \"tree_method\" : gpu_hist,\n",
    "    'max_depth': 3,\n",
    "    'enable_categorical': True,\n",
    "    \"max_depth\": 3\n",
    "}\n",
    "# training, we set the early stopping rounds parameter\n",
    "model_xgb = xgb.train(params,\n",
    "                      train, evals=[(train, \"train\"), (test, \"validation\")],\n",
    "                      num_boost_round=1000\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_xgb.best_ntree_limit)\n",
    "\n",
    "max(model_xgb.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Vizualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tested values over all trials\n",
    "\n",
    "# sns.set(style=\"whitegrid\", palette=\"Accent\")\n",
    "\n",
    "# tids = [t['tid'] for t in trials.trials]\n",
    "# n_estimators = [t['misc']['vals']['n_estimators']for t in trials]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(tids, n_estimators)\n",
    "\n",
    "# ax.legend(('n_estimators'), loc='lower right')\n",
    "# ax.set_ylabel('n_estimators over all trials')\n",
    "# ax.set_xlabel('trialIDs')\n",
    "# fig.set_size_inches(10, 5)\n",
    "\n",
    "# fig.savefig('./visualizations/tested_values.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss Improvement over Trials\n",
    "# from itertools import chain\n",
    "\n",
    "# # loss Improvement over Trials\n",
    "\n",
    "\n",
    "# def plot_reached_min_losses(trials):\n",
    "#     losses = [t['result']['loss'] for t in trials]\n",
    "#     tids = [t['tid'] for t in trials.trials]\n",
    "#     n_estimators = [t['misc']['vals']['n_estimators'] for t in trials]\n",
    "\n",
    "#     n_estimators = list(chain.from_iterable(n_estimators))\n",
    "\n",
    "#     best_loss = losses[0]\n",
    "#     points_to_plot = []\n",
    "#     points_to_plot.append(losses[0])\n",
    "#     tids_with_loss_improvement = [0]\n",
    "#     counter = 0\n",
    "#     for i in range(1, len(losses)):\n",
    "#         if losses[i] < best_loss:\n",
    "#             tid = tids[i]\n",
    "#             # print(tid)\n",
    "#             points_to_plot.append(losses[i])\n",
    "#             tids_with_loss_improvement.append(tid)\n",
    "#             best_loss = losses[i]\n",
    "\n",
    "#     # plotting with logarithmic y-scale\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "#     fig, ax = plt.subplots()\n",
    "#     # ax.set_yscale('log')\n",
    "#     ax.set_ylabel('developement of min loss')\n",
    "#     ax.set_xlabel('Trial-IDs')\n",
    "#     fig.set_size_inches(15, 5)\n",
    "#     ax.plot(tids_with_loss_improvement, points_to_plot,\n",
    "#             color=\"mediumpurple\", linestyle='-', marker='o')\n",
    "#     ax.scatter(tids, losses, color='skyblue')\n",
    "\n",
    "\n",
    "# plot_reached_min_losses(trials)\n",
    "\n",
    "# fig.savefig('./visualizations/loss_improvement.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **relicts & failed tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimizing n_estimators to reach the highest possible accuracy\n",
    "\n",
    "# # N_ESTIMATORS should be max the SEARCHSPACE so all possibilities are tried once\n",
    "# MAX_EVALS = 20 \n",
    "# SEARCH_SPACE = [hp.uniformint('n_estimators', 2000, 8000), hp.uniformint('max_depth', 0, 25)]\n",
    "\n",
    "# ### Optimizaion ##############################################################################################################\n",
    "\n",
    "\n",
    "# def cost_function(params):\n",
    "#     n_estimators = params[0]\n",
    "#     max_depth = params[1]\n",
    "#     if n_estimators == 0:\n",
    "#         return 0\n",
    "#     xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=max_depth, random_state = 0).fit(features_train, target_train)\n",
    "#     xgb_predictions = xgb_classifier.predict(features_test)\n",
    "#     xgb_accuracy = accuracy_score(target_test, xgb_predictions)\n",
    "#     return {'loss': - xgb_accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# trials = Trials()\n",
    "# best = fmin(cost_function,\n",
    "#             space=SEARCH_SPACE,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=MAX_EVALS,\n",
    "#             trials=trials)\n",
    "\n",
    "# print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best loss:  -0.871387742140362\n",
    "\n",
    "{'n_estimators': 2265.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = 5120\n",
    "# max_depth = 3\n",
    "\n",
    "# xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, objective='binary:logistic',\n",
    "#                                    tree_method='hist', eta=0.1, max_depth=max_depth).fit(features_train, target_train)\n",
    "# xgb_predictions = xgb_classifier.predict(features_test)\n",
    "# xgb_accuracy = accuracy_score(target_test, xgb_predictions)\n",
    "# print(xgb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filename = 'xgb_model.pickle'\n",
    "\n",
    "# # pickle.dump(xgb_classifier, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "# xgb_classifier.save_model('xgb_model.json')\n",
    "\n",
    "# xgb.Booster().load_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xgb.Booster({'max_depth': 3, 'n_estimators' : 5120})  # init model\n",
    "# model.load_model('xgb_model')  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage Example' #################################################################################################\n",
    "\n",
    "# xgb_classifier = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "# example_input = features_test.iloc[576]\n",
    "# # print (example_input.shape)\n",
    "\n",
    "# def make_prediction(xgb_classifier, input):\n",
    "#     prediction = xgb_classifier.predict(input.to_numpy())\n",
    "\n",
    "#     return prediction\n",
    "\n",
    "\n",
    "# prediction = make_prediction(xgb_classifier, example_input)\n",
    "# print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f891c0ef62455f9f8d0cbb2d117bcba51effc94d210f58e83104cbf88b6bc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
